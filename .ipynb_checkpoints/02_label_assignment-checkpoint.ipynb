{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 43791.96 MiB, increment: 43360.40 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# data\n",
    "clean_df=pd.read_pickle(\"/gpfs01/berens/user/rgonzalesmarquez/variables/clean_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colors by journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_coloring(journals,words_may, words_min,list_colors):\n",
    "    \"\"\"\n",
    "    Function that creates an array with colors, assigning a color to each paper depending on which words the name \n",
    "    of its journal contains. The colors that will be assigned are introduced in the function by list_colors.\n",
    "    \n",
    "    IMPORTANT REMARK: if the journal name contains two words belonging to the word list, the color of the word\n",
    "    located the latest in the list will be assigned to it (first, the first word's color is assigned and then \n",
    "    the second overwrites the first).\n",
    "    \n",
    "    Input:\n",
    "    - journals (clean_df['Journal'] in our case) - it has to be a dataframe with the journal names of the papers.\n",
    "    - words_may - list of the words starting with capital letter.\n",
    "    - words_min - list of the words with small letters.\n",
    "    - list_colors - list of all the different colors for assigning to papers.\n",
    "    \n",
    "    Output:\n",
    "    - word_colors - dict with legend of word-colors (which color has each journal-word)\n",
    "    - journal_colors - array of colors for each paper.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    N=len(words_may)\n",
    "    \n",
    "    dict_colors={}\n",
    "    word_colors={}\n",
    "    for i in range(N):\n",
    "        # I create a dictionary with the legend word-color for informative purpose\n",
    "        word_colors[words_may[i]]=list_colors[i]\n",
    "        \n",
    "        #sub1 is a string with the word in small letters and a space before it\n",
    "        sub1=words_min[i]\n",
    "        #sub2 is a string with the word starting with capital letter\n",
    "        sub2=words_may[i]\n",
    "        \n",
    "        # .find returns a -1 in the case that it didn't find the str it was looking for\n",
    "        indexes1= journals.str.find(sub1) \n",
    "        indexes2= journals.str.find(sub2)\n",
    "        \n",
    "        #containing_journals are the journals (the whole name) containing either the word in small letter or starting \n",
    "        #with capital letter\n",
    "        containing_journals=journals[(indexes1!=-1) | (indexes2!=-1)]\n",
    "        containing_journals=containing_journals.to_numpy()\n",
    "        \n",
    "        #unique_containing_j are the unique journal names from containing_journals\n",
    "        unique_containing_j=np.unique(containing_journals)\n",
    "        \n",
    "        #here we assign one color (the same to all) to each unique journal name containing the chosen word\n",
    "        for elem in unique_containing_j:\n",
    "            dict_colors[elem]=list_colors[i]\n",
    "    \n",
    "    #create colors\n",
    "    journal_colors=np.vectorize(dict_colors.get)(journals)\n",
    "    \n",
    "    #add grey to the rest of papers\n",
    "    journal_colors=np.where(journal_colors==None,'lightgrey', journal_colors)\n",
    "    journal_colors=np.where(journal_colors=='None','lightgrey', journal_colors)\n",
    "    \n",
    "    return word_colors, journal_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description of colors and words below.\n",
    "\n",
    "Colors:\n",
    "-chosen_colors: Scanpy colors selection (originally from http://godsnotwheregodsnot.blogspot.com/2013/11/kmeans-color-quantization-seeding.html)\n",
    "\n",
    "Words:\n",
    "-words_capital: Words selected manually from the 100 most relevant words using threshold 0.1.\n",
    "                They start with capital letter.\n",
    "\n",
    "-words_small: Words selected manually from the 100 most relevant words using threshold 0.1.\n",
    "                They are all in small letters.\n",
    "\"\"\"\n",
    "\n",
    "# COLORS\n",
    "chosen_colors = ['black', '#FFFF00', '#1CE6FF', '#FF34FF', '#FF4A46', '#008941', '#006FA6', '#A30059', '#7A4900', '#0000A6', \n",
    "                 '#63FFAC', '#B79762', '#004D43', '#8FB0FF', '#D16100', '#5A0007', '#BA0900', '#1B4400', '#4FC601', '#3B5DFF', '#00C2A0']\n",
    "\n",
    "\n",
    "# WORDS\n",
    "words_capital=['Cancer', 'Neuroscience', 'Cardiology', 'Ecology', 'Bioinformatics','Chemistry', 'Surgery',\n",
    "               'Biology', 'Environmental', 'Material', \n",
    "               'Cell', 'Microbiology', 'Pediatric', 'Immunology', 'Food',\n",
    "               'Psychology','Psychiatry', 'Genetics', 'Nutrition', 'Veterinary',\n",
    "               'Engineering'] \n",
    "\n",
    "words_small=[' cancer', ' neuroscience', ' cardiology', ' ecology', ' bioinformatics',' chemistry', ' surgery',\n",
    "               ' biology', ' environmental', ' material', \n",
    "               ' cell', ' microbiology', ' pediatric', ' immunology', ' food',\n",
    "               ' psychology',' psychiatry', ' genetics', ' nutrition', ' veterinary',\n",
    "               ' engineering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 53s, sys: 8.27 s, total: 6min 2s\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create the colorings\n",
    "automatic_legend, automatic_colors = automatic_coloring(clean_df['Journal'],words_capital, words_small, chosen_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "np.save(\"variables/automatic_colors\", automatic_colors)\n",
    "\n",
    "#save results\n",
    "f = open(\"variables/automatic_legend.pkl\",\"wb\")\n",
    "pickle.dump(automatic_legend,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colors by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date\n",
    "all_dates=clean_df['Date']\n",
    "unique_dates=np.unique(all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 5.85 ms, total: 121 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We do this to get the vocabulary, that divides the date strings in their different words\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "tfidf_dates = vectorizer.fit_transform(unique_dates)\n",
    "vocabulary_dates=vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1808' '1881' '1891' '1896' '1897' '1898' '1899' '1900' '1901' '1902'\n",
      " '1903' '1905' '1906' '1907' '1908' '1909' '1910' '1911' '1912' '1913'\n",
      " '1914' '1915' '1916' '1917' '1918' '1919' '1920' '1921' '1922' '1923'\n",
      " '1924' '1925' '1926' '1927' '1928' '1929' '1930' '1931' '1932' '1933'\n",
      " '1934' '1935' '1936' '1937' '1938' '1939' '1940' '1941' '1942' '1943'\n",
      " '1944' '1945' '1946' '1947' '1948' '1949' '1950' '1951' '1952' '1953'\n",
      " '1954' '1955' '1956' '1957' '1958' '1959' '1960' '1961' '1962' '1963'\n",
      " '1964' '1965' '1966' '1967' '1968' 'fall' '1969' '1970' '1971' '1972'\n",
      " '1973' '1974' '1975' '1976' '1977' '1978' '1979' '1980' '1981' '1982'\n",
      " '1983' '1984' '1985' '1986' '1987' '1988' '1989' '1990' '1991' '1992'\n",
      " '1993' '1994' '1995' 'june' 'july' '1996' '1997' '1998' '1999' '2000'\n",
      " '2004' '2001' '2002' '2003' '2005' '2006' '2007' '2008' '2009' '2010'\n",
      " '2011' '2012' '2013' '2014' '2015' 'sept' '2016' 'post' '2017' 'mary'\n",
      " 'spec' '2018' '2019' 'bima' '2020' 'jukt' '2021']\n"
     ]
    }
   ],
   "source": [
    "dates_list=list(vocabulary_dates.keys())\n",
    "len_dates_list=map(len, dates_list)\n",
    "len_dates=np.fromiter(len_dates_list, dtype=np.int64,count=len(dates_list))\n",
    "\n",
    "dates_list=np.array(dates_list)\n",
    "years=dates_list[len_dates==4]\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate manually the words that are not years\n",
    "\n",
    "years=['1808', '1881', '1891', '1896', '1897', '1898', '1899', '1900', '1901', '1902',\n",
    " '1903', '1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912', '1913',\n",
    " '1914', '1915', '1916', '1917', '1918', '1919', '1920', '1921', '1922', '1923',\n",
    " '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933',\n",
    " '1934', '1935', '1936', '1937', '1938', '1939', '1940', '1941', '1942', '1943',\n",
    " '1944', '1945', '1946', '1947', '1948', '1949', '1950', '1951', '1952', '1953',\n",
    " '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1961', '1962', '1963',\n",
    " '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972',\n",
    " '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982',\n",
    " '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992',\n",
    " '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000',\n",
    " '2004', '2001', '2002', '2003', '2005', '2006', '2007', '2008', '2009', '2010',\n",
    " '2011', '2012', '2013', '2014', '2015',  '2016', '2017', \n",
    "  '2018', '2019',  '2020',  '2021']\n",
    "\n",
    "#save\n",
    "np.save(\"variables/years\",years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicctionary years(number):color value\n",
    "\n",
    "length_interval=2021-1970\n",
    "cmap_values=np.linspace(0,1,length_interval+1)\n",
    "year_numbers_list=np.arange(1970,2022).tolist()\n",
    "\n",
    "dicc_years = dict(zip(year_numbers_list, cmap_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEDLINE started its record in 1966 and later included almost 60 thousand noteworthy papers previously published. Therefore, the majority of the papers from PubMed are post 1970, so we used a color map going from blue (1970) to yellow (2021) and all of the papers dated before 1970 were also colored in the darkest hue of blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We manually set papers dated from before 1970 to 0 (equivalent to the darkest hue).\n",
    "\n",
    "years_out=[1808, 1881, 1891, 1896, 1897, 1898, 1899,1900, 1901, 1902, 1903, 1905, 1906, 1907, 1908, \n",
    "           1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, \n",
    "           1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, \n",
    "           1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, \n",
    "           1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969] \n",
    "\n",
    "for elem in years_out:\n",
    "    dicc_years[elem]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def years_coloring(publication_date, years, color_dict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - publication_date: the dataframe column with the publication date of the paper.\n",
    "    - years: a list of all the years as strings.\n",
    "    - color_dict: it is a dictionary where you have for each year a value in between 0 and 1 for the colormap.\n",
    "    \n",
    "    Output:\n",
    "    - year_colors - array of colors for each paper.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # date_year: the year contained in the publication date for every paper is stored here\n",
    "    date_year=np.zeros(len(publication_date))\n",
    "    \n",
    "    for i in range(len(years)):\n",
    "        # .find returns a -1 in the case that it didn't find the str it was looking for\n",
    "        indexes1= publication_date.str.find(years[i]) \n",
    "        \n",
    "        date_year[indexes1!=-1]=int(years[i])\n",
    "    \n",
    "    #create colors\n",
    "    year_colors=np.vectorize(color_dict.get)(date_year)\n",
    "    \n",
    "    return year_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 26s, sys: 29.3 s, total: 16min 56s\n",
      "Wall time: 16min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "colors_per_year= years_coloring(clean_df['Date'], years, dicc_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "np.save(\"variables/colors_per_year\", colors_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
